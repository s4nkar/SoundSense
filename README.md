# ğŸ§ SoundSense  
### Vocal Anomaly Detection: Identifying Emotional Inconsistencies in Speech

SoundSense is a research project focused on detecting emotional mismatches between what is said (text) and how it's said (audio). By combining NLP and voice analysis, we aim to identify **vocal anomalies** â€” such as when someone says "I'm fine" but sounds sad.

---

## ğŸ—‚ï¸ Datasets Used

1. **CREMA-D** â€“ Crowd-sourced Emotional Multimodal Actors Dataset  
   ğŸ”— [View on Kaggle](https://www.kaggle.com/datasets/ejlok1/cremad)

2. **Emotion Dataset (Text-Based)** â€“ Twitter text samples labeled with emotional categories  
   ğŸ”— [View on Kaggle](https://www.kaggle.com/datasets/bhavikjikadara/emotions-dataset)

---

## ğŸ“Œ Project Objectives

- ğŸ™ï¸ **Voice Emotion Classification** using audio features like tone, pitch, and pace
- ğŸ§  **Text-Based Emotion Detection** using pre-trained BERT models
- âš–ï¸ **Cross-Modal Comparison** to flag inconsistencies between vocal tone and spoken content

---

## ğŸ“¬ Contact

ğŸ“© DM or email me for access to:
- The cleaned datasets  
- Trained models  

---

## ğŸ“¦ Getting Started

Clone the project:

```bash
git clone https://github.com/s4nkar/SoundSense.git
cd SoundSense
# run main.ipynb if you already have trained models
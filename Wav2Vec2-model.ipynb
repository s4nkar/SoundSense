{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e370164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchaudio\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "# from torch.optim import AdamW\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm import tqdm\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import librosa\n",
    "# import logging\n",
    "# import torch.nn as nn\n",
    "# from transformers import get_scheduler\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5409f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Constants\n",
    "# SAMPLE_RATE = 16000\n",
    "# MIN_DURATION = 0.1\n",
    "# MAX_DURATION = 10.0\n",
    "# MIN_AMPLITUDE = 1e-6\n",
    "# MAX_LENGTH = 16000\n",
    "# BATCH_SIZE = 8\n",
    "# NUM_EPOCHS = 15\n",
    "\n",
    "# # Emotion to integer mapping for CREMA-D\n",
    "# emotion_map = {\n",
    "#     \"ANG\": 0,  # Anger\n",
    "#     \"DIS\": 1,  # Disgust\n",
    "#     \"FEA\": 2,  # Fear\n",
    "#     \"HAP\": 3,  # Happy\n",
    "#     \"NEU\": 4,  # Neutral\n",
    "#     \"SAD\": 5,  # Sad\n",
    "# }\n",
    "\n",
    "# # Validate audio file\n",
    "# def validate_audio_file(file_path, processor):\n",
    "#     try:\n",
    "#         if os.path.getsize(file_path) == 0:\n",
    "#             raise ValueError(\"Empty file\")\n",
    "#         waveform, sr = torchaudio.load(file_path)\n",
    "#         if waveform.numel() == 0:\n",
    "#             raise ValueError(\"Empty waveform\")\n",
    "#         duration = waveform.shape[1] / sr\n",
    "#         if duration < MIN_DURATION or duration > MAX_DURATION:\n",
    "#             raise ValueError(f\"Invalid duration: {duration}s\")\n",
    "#         waveform_np = waveform.squeeze(0).numpy()\n",
    "#         max_amplitude = np.max(np.abs(waveform_np))\n",
    "#         mean_amplitude = np.mean(np.abs(waveform_np))\n",
    "#         if max_amplitude < MIN_AMPLITUDE:\n",
    "#             raise ValueError(f\"Waveform amplitude too low: max={max_amplitude}, mean={mean_amplitude}\")\n",
    "#         inputs = processor(waveform_np, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "#         if inputs.input_values.numel() == 0 or torch.isnan(inputs.input_values).any():\n",
    "#             raise ValueError(f\"Processor returned invalid input: numel={inputs.input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "#         logger.debug(f\"Validated {file_path}: SR={sr}, Duration={duration}s, MaxAmplitude={max_amplitude}, MeanAmplitude={mean_amplitude}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Validation failed for {file_path}: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# # Extract hand-crafted features using librosa\n",
    "# def extract_audio_features(file_path):\n",
    "#     try:\n",
    "#         y, sr = librosa.load(file_path, sr=None)\n",
    "#         mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T, axis=0)\n",
    "#         chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "#         contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "#         zcr = np.mean(librosa.feature.zero_crossing_rate(y).T, axis=0)\n",
    "#         rms = np.mean(librosa.feature.rms(y=y).T, axis=0)\n",
    "#         features = np.concatenate((mfcc, chroma, contrast, zcr, rms), axis=0)\n",
    "#         if features.shape[0] != 34:\n",
    "#             raise ValueError(f\"Expected 34 features, got {features.shape[0]}\")\n",
    "#         logger.debug(f\"Extracted hand-crafted features for {file_path}: Shape={features.shape}\")\n",
    "#         return features\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error extracting hand-crafted features for {file_path}: {e}\")\n",
    "#         return np.zeros(34)\n",
    "\n",
    "# # Extract Wav2Vec2 features\n",
    "# def extract_wav2vec2_features(file_path, processor, sample_rate=16000, max_length=16000):\n",
    "#     try:\n",
    "#         waveform, sr = torchaudio.load(file_path)\n",
    "#         if waveform.numel() == 0:\n",
    "#             raise ValueError(\"Empty waveform\")\n",
    "#         if sr != sample_rate:\n",
    "#             waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "#         waveform_np = waveform.squeeze(0).numpy()\n",
    "#         inputs = processor(waveform_np, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "#         input_values = inputs['input_values'].squeeze(0)\n",
    "#         attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "#         if input_values.numel() == 0 or torch.isnan(input_values).any():\n",
    "#             raise ValueError(f\"Processor returned invalid input: numel={input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "#         logger.debug(f\"Extracted Wav2Vec2 features for {file_path}: InputShape={input_values.shape}, MaskShape={attention_mask.shape}\")\n",
    "#         return input_values, attention_mask\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error extracting Wav2Vec2 features for {file_path}: {e}\")\n",
    "#         return torch.zeros(max_length), torch.zeros(max_length)\n",
    "\n",
    "# # Custom Dataset\n",
    "# class CREMADataset(Dataset):\n",
    "#     def __init__(self, audio_dir, processor, emotion_map, file_paths=None, labels=None, sample_rate=16000):\n",
    "#         self.audio_dir = audio_dir\n",
    "#         self.processor = processor\n",
    "#         self.emotion_map = emotion_map\n",
    "#         self.sample_rate = sample_rate\n",
    "#         self.file_paths = file_paths or []\n",
    "#         self.labels = labels or []\n",
    "#         self.invalid_files = []\n",
    "\n",
    "#         if not file_paths or not labels:\n",
    "#             for file_name in os.listdir(audio_dir):\n",
    "#                 if file_name.endswith(\".wav\"):\n",
    "#                     file_path = os.path.join(audio_dir, file_name)\n",
    "#                     parts = file_name.split('_')\n",
    "#                     if len(parts) < 3:\n",
    "#                         logger.warning(f\"Invalid filename format: {file_name}\")\n",
    "#                         continue\n",
    "#                     emotion = parts[2]\n",
    "#                     label = self.emotion_map.get(emotion, -1)\n",
    "#                     if label != -1 and validate_audio_file(file_path, processor):\n",
    "#                         self.file_paths.append(file_path)\n",
    "#                         self.labels.append(label)\n",
    "#                     else:\n",
    "#                         self.invalid_files.append(file_path)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         file_path = os.path.normpath(self.file_paths[idx])\n",
    "#         label = self.labels[idx]\n",
    "#         try:\n",
    "#             wav2vec_features, attention_mask = extract_wav2vec2_features(file_path, self.processor, self.sample_rate, MAX_LENGTH)\n",
    "#             hand_crafted_features = extract_audio_features(file_path)\n",
    "#             return {\n",
    "#                 'input_values': wav2vec_features.clone().detach(),\n",
    "#                 'attention_mask': attention_mask.clone().detach(),\n",
    "#                 'labels': torch.tensor(label, dtype=torch.long),\n",
    "#                 'hand_crafted_features': torch.tensor(hand_crafted_features, dtype=torch.float)\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error processing {file_path}: {e}\")\n",
    "#             self.invalid_files.append(file_path)\n",
    "#             return None\n",
    "\n",
    "# # Custom Model\n",
    "# class EmotionRecognitionModel(nn.Module):\n",
    "#     def __init__(self, wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=6):\n",
    "#         super(EmotionRecognitionModel, self).__init__()\n",
    "#         self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "#             wav2vec_model_name, num_labels=num_labels, ignore_mismatched_sizes=True, output_hidden_states=True\n",
    "#         )\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(768 + 34, 256),  # 768 from Wav2Vec2 hidden states + 34 from hand-crafted features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_values, attention_mask, hand_crafted_features):\n",
    "#         outputs = self.wav2vec2(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "#         hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, 768)\n",
    "#         pooled_features = hidden_states.mean(dim=1)  # Shape: (batch_size, 768)\n",
    "#         combined_features = torch.cat((pooled_features, hand_crafted_features), dim=1)  # Shape: (batch_size, 768 + 34)\n",
    "#         logits = self.classifier(combined_features)\n",
    "#         return logits\n",
    "\n",
    "# # Load processor and dataset\n",
    "# processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "# audio_dir = \"audio-emotion-dataset2\"\n",
    "# dataset = CREMADataset(audio_dir=audio_dir, processor=processor, emotion_map=emotion_map)\n",
    "\n",
    "# # Split dataset\n",
    "# train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "#     dataset.file_paths, dataset.labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "# train_dataset = CREMADataset(audio_dir, processor, emotion_map, train_file_paths, train_labels)\n",
    "# val_dataset = CREMADataset(audio_dir, processor, emotion_map, val_file_paths, val_labels)\n",
    "\n",
    "# # Filter out None samples\n",
    "# def filter_dataset(dataset):\n",
    "#     valid_indices = []\n",
    "#     valid_data = []\n",
    "#     for idx in range(len(dataset)):\n",
    "#         item = dataset[idx]\n",
    "#         if item is not None:\n",
    "#             valid_indices.append(idx)\n",
    "#             valid_data.append(item)\n",
    "#     if not valid_data:\n",
    "#         logger.error(f\"No valid data after filtering. Invalid files: {dataset.invalid_files}\")\n",
    "#         raise ValueError(f\"No valid data after filtering. Check invalid files: {dataset.invalid_files[:10]}...\")\n",
    "#     dataset.file_paths = [dataset.file_paths[i] for i in valid_indices]\n",
    "#     dataset.labels = [dataset.labels[i] for i in valid_indices]\n",
    "#     dataset.invalid_files = list(set(dataset.invalid_files))\n",
    "#     logger.warning(f\"Invalid files: {dataset.invalid_files}\")\n",
    "#     return valid_data\n",
    "\n",
    "# train_dataset = filter_dataset(train_dataset)\n",
    "# val_dataset = filter_dataset(val_dataset)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# def collate_fn(batch):\n",
    "#     batch = [item for item in batch if item is not None]\n",
    "#     if not batch:\n",
    "#         return None\n",
    "#     input_values = [item['input_values'] for item in batch]\n",
    "#     attention_mask = [item['attention_mask'] for item in batch]\n",
    "#     labels = [item['labels'] for item in batch]\n",
    "#     hand_crafted_features = [item['hand_crafted_features'] for item in batch]\n",
    "#     input_values = pad_sequence(input_values, batch_first=True, padding_value=0)\n",
    "#     attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "#     labels = torch.stack(labels, dim=0)\n",
    "#     hand_crafted_features = torch.stack(hand_crafted_features, dim=0)\n",
    "#     return {\n",
    "#         'input_values': input_values,\n",
    "#         'attention_mask': attention_mask,\n",
    "#         'labels': labels,\n",
    "#         'hand_crafted_features': hand_crafted_features\n",
    "#     }\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f437fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# label_counts = Counter(train_labels + val_labels)  # or test_labels if available\n",
    "# print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fb3a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchaudio\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "# from torch.optim import AdamW\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from tqdm import tqdm\n",
    "# import librosa\n",
    "# import logging\n",
    "# import torch.nn as nn\n",
    "# from transformers import get_scheduler\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Constants\n",
    "# SAMPLE_RATE = 16000\n",
    "# MIN_DURATION = 0.1\n",
    "# MAX_DURATION = 10.0\n",
    "# MIN_AMPLITUDE = 1e-6\n",
    "# MAX_LENGTH = 16000\n",
    "# BATCH_SIZE = 8\n",
    "# NUM_EPOCHS = 15\n",
    "\n",
    "# # Emotion to integer mapping for CREMA-D\n",
    "# # emotion_map = {\n",
    "# #     \"ANG\": 0,  # Anger\n",
    "# #     \"DIS\": 1,  # Disgust\n",
    "# #     \"FEA\": 2,  # Fear\n",
    "# #     \"HAP\": 3,  # Happy\n",
    "# #     \"NEU\": 4,  # Neutral\n",
    "# #     \"SAD\": 5,  # Sad\n",
    "# # }\n",
    "\n",
    "# emotion_map = {'HAP': 0, 'SAD': 1, 'ANG': 2, 'FEA': 3, 'DIS': 4, 'NEU': 5}\n",
    "\n",
    "# # Validate audio file\n",
    "# def validate_audio_file(file_path, processor):\n",
    "#     try:\n",
    "#         if os.path.getsize(file_path) == 0:\n",
    "#             raise ValueError(\"Empty file\")\n",
    "#         waveform, sr = torchaudio.load(file_path)\n",
    "#         if waveform.numel() == 0:\n",
    "#             raise ValueError(\"Empty waveform\")\n",
    "#         duration = waveform.shape[1] / sr\n",
    "#         if duration < MIN_DURATION or duration > MAX_DURATION:\n",
    "#             raise ValueError(f\"Invalid duration: {duration}s\")\n",
    "#         waveform_np = waveform.squeeze(0).numpy()\n",
    "#         max_amplitude = np.max(np.abs(waveform_np))\n",
    "#         mean_amplitude = np.mean(np.abs(waveform_np))\n",
    "#         if max_amplitude < MIN_AMPLITUDE:\n",
    "#             raise ValueError(f\"Waveform amplitude too low: max={max_amplitude}, mean={mean_amplitude}\")\n",
    "#         inputs = processor(waveform_np, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "#         if inputs.input_values.numel() == 0 or torch.isnan(inputs.input_values).any():\n",
    "#             raise ValueError(f\"Processor returned invalid input: numel={inputs.input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "#         logger.debug(f\"Validated {file_path}: SR={sr}, Duration={duration}s, MaxAmplitude={max_amplitude}, MeanAmplitude={mean_amplitude}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Validation failed for {file_path}: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# # Extract hand-crafted features using librosa\n",
    "# def extract_audio_features(file_path):\n",
    "#     try:\n",
    "#         y, sr = librosa.load(file_path, sr=None)\n",
    "#         mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T, axis=0)\n",
    "#         chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "#         contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "#         zcr = np.mean(librosa.feature.zero_crossing_rate(y).T, axis=0)\n",
    "#         rms = np.mean(librosa.feature.rms(y=y).T, axis=0)\n",
    "#         features = np.concatenate((mfcc, chroma, contrast, zcr, rms), axis=0)\n",
    "#         if features.shape[0] != 34:\n",
    "#             raise ValueError(f\"Expected 34 features, got {features.shape[0]}\")\n",
    "#         logger.debug(f\"Extracted hand-crafted features for {file_path}: Shape={features.shape}\")\n",
    "#         return features\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error extracting hand-crafted features for {file_path}: {e}\")\n",
    "#         return np.zeros(34)\n",
    "\n",
    "# # Extract Wav2Vec2 features\n",
    "# def extract_wav2vec2_features(file_path, processor, sample_rate=16000, max_length=16000):\n",
    "#     try:\n",
    "#         waveform, sr = torchaudio.load(file_path)\n",
    "#         if waveform.numel() == 0:\n",
    "#             raise ValueError(\"Empty waveform\")\n",
    "#         if sr != sample_rate:\n",
    "#             waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "#         waveform_np = waveform.squeeze(0).numpy()\n",
    "#         inputs = processor(waveform_np, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "#         input_values = inputs['input_values'].squeeze(0)\n",
    "#         attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "#         if input_values.numel() == 0 or torch.isnan(input_values).any():\n",
    "#             raise ValueError(f\"Processor returned invalid input: numel={input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "#         logger.debug(f\"Extracted Wav2Vec2 features for {file_path}: InputShape={input_values.shape}, MaskShape={attention_mask.shape}\")\n",
    "#         return input_values, attention_mask\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error extracting Wav2Vec2 features for {file_path}: {e}\")\n",
    "#         return torch.zeros(max_length), torch.zeros(max_length)\n",
    "\n",
    "# # Custom Dataset\n",
    "# class CREMADataset(Dataset):\n",
    "#     def __init__(self, audio_dir, processor, emotion_map, file_paths=None, labels=None, sample_rate=16000):\n",
    "#         self.audio_dir = audio_dir\n",
    "#         self.processor = processor\n",
    "#         self.emotion_map = emotion_map\n",
    "#         self.sample_rate = sample_rate\n",
    "#         self.file_paths = file_paths or []\n",
    "#         self.labels = labels or []\n",
    "#         self.invalid_files = []\n",
    "\n",
    "#         if not file_paths or not labels:\n",
    "#             for file_name in os.listdir(audio_dir):\n",
    "#                 if file_name.endswith(\".wav\"):\n",
    "#                     file_path = os.path.join(audio_dir, file_name)\n",
    "#                     parts = file_name.split('_')\n",
    "#                     if len(parts) < 3:\n",
    "#                         logger.warning(f\"Invalid filename format: {file_name}\")\n",
    "#                         continue\n",
    "#                     emotion = parts[2]\n",
    "#                     label = self.emotion_map.get(emotion, -1)\n",
    "#                     if label != -1 and validate_audio_file(file_path, processor):\n",
    "#                         self.file_paths.append(file_path)\n",
    "#                         self.labels.append(label)\n",
    "#                     else:\n",
    "#                         self.invalid_files.append(file_path)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         file_path = os.path.normpath(self.file_paths[idx])\n",
    "#         label = self.labels[idx]\n",
    "#         try:\n",
    "#             wav2vec_features, attention_mask = extract_wav2vec2_features(file_path, self.processor, self.sample_rate, MAX_LENGTH)\n",
    "#             hand_crafted_features = extract_audio_features(file_path)\n",
    "#             return {\n",
    "#                 'input_values': wav2vec_features.clone().detach(),\n",
    "#                 'attention_mask': attention_mask.clone().detach(),\n",
    "#                 'labels': torch.tensor(label, dtype=torch.long),\n",
    "#                 'hand_crafted_features': torch.tensor(hand_crafted_features, dtype=torch.float)\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error processing {file_path}: {e}\")\n",
    "#             self.invalid_files.append(file_path)\n",
    "#             return None\n",
    "\n",
    "# # Custom Model\n",
    "# class EmotionRecognitionModel(nn.Module):\n",
    "#     def __init__(self, wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=6):\n",
    "#         super(EmotionRecognitionModel, self).__init__()\n",
    "#         self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "#             wav2vec_model_name, num_labels=num_labels, ignore_mismatched_sizes=True, output_hidden_states=True\n",
    "#         )\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(768 + 34, 256),  # 768 from Wav2Vec2 hidden states + 34 from hand-crafted features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_values, attention_mask, hand_crafted_features):\n",
    "#         outputs = self.wav2vec2(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "#         hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, 768)\n",
    "#         pooled_features = hidden_states.mean(dim=1)  # Shape: (batch_size, 768)\n",
    "#         combined_features = torch.cat((pooled_features, hand_crafted_features), dim=1)  # Shape: (batch_size, 768 + 34)\n",
    "#         logits = self.classifier(combined_features)\n",
    "#         return logits\n",
    "\n",
    "# # Load processor and dataset\n",
    "# processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "# audio_dir = \"audio-emotion-dataset2\"\n",
    "# dataset = CREMADataset(audio_dir=audio_dir, processor=processor, emotion_map=emotion_map)\n",
    "\n",
    "# # Split dataset\n",
    "# train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "#     dataset.file_paths, dataset.labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "# train_dataset = CREMADataset(audio_dir, processor, emotion_map, train_file_paths, train_labels)\n",
    "# val_dataset = CREMADataset(audio_dir, processor, emotion_map, val_file_paths, val_labels)\n",
    "\n",
    "# # Filter out None samples\n",
    "# def filter_dataset(dataset):\n",
    "#     valid_indices = []\n",
    "#     valid_data = []\n",
    "#     for idx in range(len(dataset)):\n",
    "#         item = dataset[idx]\n",
    "#         if item is not None:\n",
    "#             valid_indices.append(idx)\n",
    "#             valid_data.append(item)\n",
    "#     if not valid_data:\n",
    "#         logger.error(f\"No valid data after filtering. Invalid files: {dataset.invalid_files}\")\n",
    "#         raise ValueError(f\"No valid data after filtering. Check invalid files: {dataset.invalid_files[:10]}...\")\n",
    "#     dataset.file_paths = [dataset.file_paths[i] for i in valid_indices]\n",
    "#     dataset.labels = [dataset.labels[i] for i in valid_indices]\n",
    "#     dataset.invalid_files = list(set(dataset.invalid_files))\n",
    "#     logger.warning(f\"Invalid files: {dataset.invalid_files}\")\n",
    "#     return valid_data\n",
    "\n",
    "# train_dataset = filter_dataset(train_dataset)\n",
    "# val_dataset = filter_dataset(val_dataset)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# def collate_fn(batch):\n",
    "#     batch = [item for item in batch if item is not None]\n",
    "#     if not batch:\n",
    "#         return None\n",
    "#     input_values = [item['input_values'] for item in batch]\n",
    "#     attention_mask = [item['attention_mask'] for item in batch]\n",
    "#     labels = [item['labels'] for item in batch]\n",
    "#     hand_crafted_features = [item['hand_crafted_features'] for item in batch]\n",
    "#     input_values = pad_sequence(input_values, batch_first=True, padding_value=0)\n",
    "#     attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "#     labels = torch.stack(labels, dim=0)\n",
    "#     hand_crafted_features = torch.stack(hand_crafted_features, dim=0)\n",
    "#     return {\n",
    "#         'input_values': input_values,\n",
    "#         'attention_mask': attention_mask,\n",
    "#         'labels': labels,\n",
    "#         'hand_crafted_features': hand_crafted_features\n",
    "#     }\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# # Initialize model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = EmotionRecognitionModel(wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=len(emotion_map))\n",
    "# model.to(device)\n",
    "\n",
    "# # Training setup\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=num_training_steps\n",
    "# )\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# # Training loop\n",
    "# best_val_accuracy = 0.0\n",
    "# best_model_path = \"wav2vec2_emotion_classifier_best_v3\"\n",
    "# os.makedirs(best_model_path, exist_ok=True)\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct_preds = 0\n",
    "#     total_preds = 0\n",
    "#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n",
    "#         if batch is None:\n",
    "#             continue\n",
    "#         batch = {key: value.to(device) for key, value in batch.items()}\n",
    "#         inputs = batch['input_values']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['labels']\n",
    "#         hand_crafted_features = batch['hand_crafted_features']\n",
    "#         logger.debug(f\"Batch shapes: input_values={inputs.shape}, attention_mask={attention_mask.shape}, hand_crafted_features={hand_crafted_features.shape}\")\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs, attention_mask=attention_mask, hand_crafted_features=hand_crafted_features)\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         total_loss += loss.item()\n",
    "#         preds = torch.argmax(outputs, dim=-1)\n",
    "#         correct_preds += (preds == labels).sum().item()\n",
    "#         total_preds += labels.size(0)\n",
    "#     train_accuracy = correct_preds / total_preds\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "#     logger.info(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "#     # Validation loop\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     correct_preds = 0\n",
    "#     total_preds = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Validation\"):\n",
    "#             if batch is None:\n",
    "#                 continue\n",
    "#             batch = {key: value.to(device) for key, value in batch.items()}\n",
    "#             inputs = batch['input_values']\n",
    "#             attention_mask = batch['attention_mask']\n",
    "#             labels = batch['labels']\n",
    "#             hand_crafted_features = batch['hand_crafted_features']\n",
    "#             outputs = model(inputs, attention_mask=attention_mask, hand_crafted_features=hand_crafted_features)\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "#             preds = torch.argmax(outputs, dim=-1)\n",
    "#             correct_preds += (preds == labels).sum().item()\n",
    "#             total_preds += labels.size(0)\n",
    "#     val_accuracy = correct_preds / total_preds\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "#     logger.info(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#     # Save best model\n",
    "#     if val_accuracy > best_val_accuracy:\n",
    "#         best_val_accuracy = val_accuracy\n",
    "#         torch.save(model.state_dict(), os.path.join(best_model_path, \"model.pt\"))\n",
    "#         processor.save_pretrained(best_model_path)\n",
    "#         logger.info(f\"Saved best model at epoch {epoch+1} with val accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# # Predict emotion for a single file\n",
    "# def predict_emotion(file_path, model, processor):\n",
    "#     model.eval()\n",
    "#     try:\n",
    "#         wav2vec_features, attention_mask = extract_wav2vec2_features(file_path, processor, SAMPLE_RATE, MAX_LENGTH)\n",
    "#         hand_crafted_features = extract_audio_features(file_path)\n",
    "#         inputs = {\n",
    "#             'input_values': wav2vec_features.unsqueeze(0).to(device),\n",
    "#             'attention_mask': attention_mask.unsqueeze(0).to(device),\n",
    "#             'hand_crafted_features': torch.tensor(hand_crafted_features, dtype=torch.float).unsqueeze(0).to(device)\n",
    "#         }\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             pred = torch.argmax(outputs, dim=-1).item()\n",
    "#         return {v: k for k, v in emotion_map.items()}[pred]\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error predicting {file_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Test prediction\n",
    "# sample_file = os.path.join(audio_dir, \"1001_DFA_ANG_XX.wav\")\n",
    "# logger.info(f\"Predicted emotion for {sample_file}: {predict_emotion(sample_file, model, processor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618880dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\transformers\\configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Epoch 1/15:   0%|          | 2/744 [00:15<1:33:48,  7.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 136\u001b[0m\n\u001b[0;32m    134\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_values)\n\u001b[0;32m    135\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n\u001b[1;32m--> 136\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    138\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_LENGTH = SAMPLE_RATE * 10  # 10 seconds\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# Emotion mapping\n",
    "emotion_map = {'HAP': 0, 'SAD': 1, 'ANG': 2, 'FEA': 3, 'DIS': 4, 'NEU': 5}\n",
    "\n",
    "# Feature extractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Dataset class\n",
    "class CREMADataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load and preprocess audio\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "        waveform = waveform.squeeze(0)\n",
    "\n",
    "        # Extract features\n",
    "        inputs = feature_extractor(waveform.numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n",
    "        input_values = inputs.input_values.squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_values': input_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Model definition\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.wav2vec2.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load data\n",
    "audio_dir = \"audio-emotion-dataset2\"\n",
    "file_paths = []\n",
    "labels = []\n",
    "for file_name in os.listdir(audio_dir):\n",
    "    if file_name.endswith(\".wav\"):\n",
    "        parts = file_name.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            emotion = parts[2]\n",
    "            if emotion in emotion_map:\n",
    "                file_paths.append(os.path.join(audio_dir, file_name))\n",
    "                labels.append(emotion_map[emotion])\n",
    "\n",
    "# Split data\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CREMADataset(train_paths, train_labels)\n",
    "val_dataset = CREMADataset(val_paths, val_labels)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_values = [item['input_values'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    input_values = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "    return {'input_values': input_values, 'labels': labels}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmotionRecognitionModel(num_labels=len(emotion_map))\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_values)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    logger.info(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Train Accuracy={train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_values)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    logger.info(f\"Epoch {epoch+1}: Validation Accuracy={val_accuracy:.4f}\")\n",
    "    logger.info(\"Classification Report:\")\n",
    "    logger.info(\"\\n\" + classification_report(all_labels, all_preds, target_names=emotion_map.keys()))\n",
    "\n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        # torch.save(model.state_dict(), \"Wav2Vec2/best_model.pt\")\n",
    "        logger.info(f\"Saved best model at epoch {epoch+1} with validation accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045567ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmotionRecognitionModel' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir)\n\u001b[0;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, model_name)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(model_path)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSave model and weights at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model_path)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Save the model to disk\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EmotionRecognitionModel' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the entire model directly\n",
    "# model = torch.load('Wav2Vec2/best_model.pt')\n",
    "\n",
    "# Save model and weights\n",
    "model_name = 'Wav2Vec2_Emotion_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'Wav2Vec2')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Save model and weights at %s ' % model_path)\n",
    "\n",
    "# Save the model to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"Wav2Vec2/Wav2Vec2_model_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032996e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading json and model architecture \n",
    "json_file = open('Wav2Vec2/Wav2Vec2_model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Wav2Vec2/Wav2Vec2_Emotion_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Keras optimiser\n",
    "# opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1],score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae05d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\transformers\\configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_15260\\2884683402.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"Wav2Vec2/best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmotionRecognitionModel(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Model\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.wav2vec2.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "emotion_map = {'HAP': 0, 'SAD': 1, 'ANG': 2, 'FEA': 3, 'DIS': 4, 'NEU': 5}\n",
    "\n",
    "    \n",
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# First load the model architecture (config will be default or custom if you have config.json)\n",
    "model = EmotionRecognitionModel(num_labels=len(emotion_map))\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"Wav2Vec2/best_model_v1.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577348d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wav2Vec2 evaluation saved under key 'wav2vec2' in evaluation_results.json\n",
      "Validation Accuracy: 78.11%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "# Inference loop\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        logits = model(input_values=input_values)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "report = classification_report(true_labels, pred_labels, target_names=emotion_map.keys(), output_dict=True)\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Build result dictionary\n",
    "wav2vec_results = {\n",
    "    \"accuracy\": accuracy * 100,\n",
    "    \"precision_macro\": report[\"macro avg\"][\"precision\"] * 100,\n",
    "    \"recall_macro\": report[\"macro avg\"][\"recall\"] * 100,\n",
    "    \"f1_macro\": report[\"macro avg\"][\"f1-score\"] * 100,\n",
    "    \"per_class_accuracy\": {\n",
    "        f\"Class {i} ({list(emotion_map.keys())[i]})\": float(acc * 100)\n",
    "        for i, acc in enumerate(per_class_accuracy)\n",
    "    },\n",
    "    \"confusion_matrix\": cm.tolist()\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "json_path = \"evaluation_results.json\"\n",
    "if os.path.exists(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        all_results = json.load(f)\n",
    "else:\n",
    "    all_results = {}\n",
    "\n",
    "all_results[\"wav2vec2\"] = wav2vec_results\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=4)\n",
    "\n",
    "print(f\" Wav2Vec2 evaluation saved under key 'wav2vec2' in {json_path}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "# best  Validation Accuracy: 78.11%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5d8614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7779164489960447"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "val_f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b55f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAP       0.82      0.81      0.81       259\n",
      "         SAD       0.78      0.59      0.67       269\n",
      "         ANG       0.84      0.93      0.88       237\n",
      "         FEA       0.73      0.76      0.74       246\n",
      "         DIS       0.79      0.76      0.78       268\n",
      "         NEU       0.73      0.87      0.79       210\n",
      "\n",
      "    accuracy                           0.78      1489\n",
      "   macro avg       0.78      0.79      0.78      1489\n",
      "weighted avg       0.78      0.78      0.78      1489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [label for label, idx in sorted(emotion_map.items(), key=lambda x: x[1])]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75cb91a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "labels = list(emotion_map.values()) \n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68ab83ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation Metrics on Validation Set:\n",
      " Accuracy:  0.1672\n",
      " Precision: 0.3975\n",
      " Recall:    0.1672\n",
      " F1 Score:  0.1087\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAP       0.16      0.12      0.14       259\n",
      "         SAD       0.65      0.05      0.09       269\n",
      "         ANG       0.24      0.04      0.07       237\n",
      "         FEA       1.00      0.00      0.01       246\n",
      "         DIS       0.14      0.08      0.10       268\n",
      "         NEU       0.16      0.82      0.27       210\n",
      "\n",
      "    accuracy                           0.17      1489\n",
      "   macro avg       0.39      0.19      0.11      1489\n",
      "weighted avg       0.40      0.17      0.11      1489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Evaluation on the validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits = model(input_values)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average='weighted'\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n Evaluation Metrics on Validation Set:\")\n",
    "print(f\" Accuracy:  {accuracy:.4f}\")\n",
    "print(f\" Precision: {precision:.4f}\")\n",
    "print(f\" Recall:    {recall:.4f}\")\n",
    "print(f\" F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Detailed per-class breakdown\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=emotion_map.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e370164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import librosa\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from transformers import get_scheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5409f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:38:50,418 - WARNING - Invalid files: []\n",
      "2025-05-05 15:39:10,755 - WARNING - Invalid files: []\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 16000\n",
    "MIN_DURATION = 0.1\n",
    "MAX_DURATION = 10.0\n",
    "MIN_AMPLITUDE = 1e-6\n",
    "MAX_LENGTH = 16000\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "# Emotion to integer mapping for CREMA-D\n",
    "emotion_map = {\n",
    "    \"ANG\": 0,  # Anger\n",
    "    \"DIS\": 1,  # Disgust\n",
    "    \"FEA\": 2,  # Fear\n",
    "    \"HAP\": 3,  # Happy\n",
    "    \"NEU\": 4,  # Neutral\n",
    "    \"SAD\": 5,  # Sad\n",
    "}\n",
    "\n",
    "# Validate audio file\n",
    "def validate_audio_file(file_path, processor):\n",
    "    try:\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            raise ValueError(\"Empty file\")\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if waveform.numel() == 0:\n",
    "            raise ValueError(\"Empty waveform\")\n",
    "        duration = waveform.shape[1] / sr\n",
    "        if duration < MIN_DURATION or duration > MAX_DURATION:\n",
    "            raise ValueError(f\"Invalid duration: {duration}s\")\n",
    "        waveform_np = waveform.squeeze(0).numpy()\n",
    "        max_amplitude = np.max(np.abs(waveform_np))\n",
    "        mean_amplitude = np.mean(np.abs(waveform_np))\n",
    "        if max_amplitude < MIN_AMPLITUDE:\n",
    "            raise ValueError(f\"Waveform amplitude too low: max={max_amplitude}, mean={mean_amplitude}\")\n",
    "        inputs = processor(waveform_np, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "        if inputs.input_values.numel() == 0 or torch.isnan(inputs.input_values).any():\n",
    "            raise ValueError(f\"Processor returned invalid input: numel={inputs.input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "        logger.debug(f\"Validated {file_path}: SR={sr}, Duration={duration}s, MaxAmplitude={max_amplitude}, MeanAmplitude={mean_amplitude}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Validation failed for {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Extract hand-crafted features using librosa\n",
    "def extract_audio_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T, axis=0)\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y).T, axis=0)\n",
    "        rms = np.mean(librosa.feature.rms(y=y).T, axis=0)\n",
    "        features = np.concatenate((mfcc, chroma, contrast, zcr, rms), axis=0)\n",
    "        if features.shape[0] != 34:\n",
    "            raise ValueError(f\"Expected 34 features, got {features.shape[0]}\")\n",
    "        logger.debug(f\"Extracted hand-crafted features for {file_path}: Shape={features.shape}\")\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting hand-crafted features for {file_path}: {e}\")\n",
    "        return np.zeros(34)\n",
    "\n",
    "# Extract Wav2Vec2 features\n",
    "def extract_wav2vec2_features(file_path, processor, sample_rate=16000, max_length=16000):\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if waveform.numel() == 0:\n",
    "            raise ValueError(\"Empty waveform\")\n",
    "        if sr != sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "        waveform_np = waveform.squeeze(0).numpy()\n",
    "        inputs = processor(waveform_np, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        input_values = inputs['input_values'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        if input_values.numel() == 0 or torch.isnan(input_values).any():\n",
    "            raise ValueError(f\"Processor returned invalid input: numel={input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "        logger.debug(f\"Extracted Wav2Vec2 features for {file_path}: InputShape={input_values.shape}, MaskShape={attention_mask.shape}\")\n",
    "        return input_values, attention_mask\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting Wav2Vec2 features for {file_path}: {e}\")\n",
    "        return torch.zeros(max_length), torch.zeros(max_length)\n",
    "\n",
    "# Custom Dataset\n",
    "class CREMADataset(Dataset):\n",
    "    def __init__(self, audio_dir, processor, emotion_map, file_paths=None, labels=None, sample_rate=16000):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.processor = processor\n",
    "        self.emotion_map = emotion_map\n",
    "        self.sample_rate = sample_rate\n",
    "        self.file_paths = file_paths or []\n",
    "        self.labels = labels or []\n",
    "        self.invalid_files = []\n",
    "\n",
    "        if not file_paths or not labels:\n",
    "            for file_name in os.listdir(audio_dir):\n",
    "                if file_name.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(audio_dir, file_name)\n",
    "                    parts = file_name.split('_')\n",
    "                    if len(parts) < 3:\n",
    "                        logger.warning(f\"Invalid filename format: {file_name}\")\n",
    "                        continue\n",
    "                    emotion = parts[2]\n",
    "                    label = self.emotion_map.get(emotion, -1)\n",
    "                    if label != -1 and validate_audio_file(file_path, processor):\n",
    "                        self.file_paths.append(file_path)\n",
    "                        self.labels.append(label)\n",
    "                    else:\n",
    "                        self.invalid_files.append(file_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.normpath(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            wav2vec_features, attention_mask = extract_wav2vec2_features(file_path, self.processor, self.sample_rate, MAX_LENGTH)\n",
    "            hand_crafted_features = extract_audio_features(file_path)\n",
    "            return {\n",
    "                'input_values': wav2vec_features.clone().detach(),\n",
    "                'attention_mask': attention_mask.clone().detach(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long),\n",
    "                'hand_crafted_features': torch.tensor(hand_crafted_features, dtype=torch.float)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {file_path}: {e}\")\n",
    "            self.invalid_files.append(file_path)\n",
    "            return None\n",
    "\n",
    "# Custom Model\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=6):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            wav2vec_model_name, num_labels=num_labels, ignore_mismatched_sizes=True, output_hidden_states=True\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 34, 256),  # 768 from Wav2Vec2 hidden states + 34 from hand-crafted features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask, hand_crafted_features):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, 768)\n",
    "        pooled_features = hidden_states.mean(dim=1)  # Shape: (batch_size, 768)\n",
    "        combined_features = torch.cat((pooled_features, hand_crafted_features), dim=1)  # Shape: (batch_size, 768 + 34)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits\n",
    "\n",
    "# Load processor and dataset\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "audio_dir = \"audio-emotion-dataset2\"\n",
    "dataset = CREMADataset(audio_dir=audio_dir, processor=processor, emotion_map=emotion_map)\n",
    "\n",
    "# Split dataset\n",
    "train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "    dataset.file_paths, dataset.labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_dataset = CREMADataset(audio_dir, processor, emotion_map, train_file_paths, train_labels)\n",
    "val_dataset = CREMADataset(audio_dir, processor, emotion_map, val_file_paths, val_labels)\n",
    "\n",
    "# Filter out None samples\n",
    "def filter_dataset(dataset):\n",
    "    valid_indices = []\n",
    "    valid_data = []\n",
    "    for idx in range(len(dataset)):\n",
    "        item = dataset[idx]\n",
    "        if item is not None:\n",
    "            valid_indices.append(idx)\n",
    "            valid_data.append(item)\n",
    "    if not valid_data:\n",
    "        logger.error(f\"No valid data after filtering. Invalid files: {dataset.invalid_files}\")\n",
    "        raise ValueError(f\"No valid data after filtering. Check invalid files: {dataset.invalid_files[:10]}...\")\n",
    "    dataset.file_paths = [dataset.file_paths[i] for i in valid_indices]\n",
    "    dataset.labels = [dataset.labels[i] for i in valid_indices]\n",
    "    dataset.invalid_files = list(set(dataset.invalid_files))\n",
    "    logger.warning(f\"Invalid files: {dataset.invalid_files}\")\n",
    "    return valid_data\n",
    "\n",
    "train_dataset = filter_dataset(train_dataset)\n",
    "val_dataset = filter_dataset(val_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    input_values = [item['input_values'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    hand_crafted_features = [item['hand_crafted_features'] for item in batch]\n",
    "    input_values = pad_sequence(input_values, batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    hand_crafted_features = torch.stack(hand_crafted_features, dim=0)\n",
    "    return {\n",
    "        'input_values': input_values,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'hand_crafted_features': hand_crafted_features\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f437fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1271, 3: 1271, 2: 1271, 0: 1271, 5: 1270, 4: 1087})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(train_labels + val_labels)  # or test_labels if available\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fb3a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:22:34,519 - WARNING - Invalid files: []\n",
      "2025-05-04 15:22:56,456 - WARNING - Invalid files: []\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Training: 100%|██████████| 744/744 [07:55<00:00,  1.56it/s]\n",
      "2025-05-04 15:30:53,649 - INFO - Epoch 1 - Train Loss: 3.8642, Train Accuracy: 0.1725\n",
      "Epoch 1 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.64it/s]\n",
      "2025-05-04 15:32:04,592 - INFO - Epoch 1 - Validation Loss: 1.6205, Validation Accuracy: 0.3224\n",
      "2025-05-04 15:32:05,031 - INFO - Saved best model at epoch 1 with val accuracy: 0.3224\n",
      "Epoch 2 - Training: 100%|██████████| 744/744 [07:50<00:00,  1.58it/s]\n",
      "2025-05-04 15:39:55,150 - INFO - Epoch 2 - Train Loss: 2.4615, Train Accuracy: 0.2387\n",
      "Epoch 2 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.64it/s]\n",
      "2025-05-04 15:41:06,117 - INFO - Epoch 2 - Validation Loss: 1.4800, Validation Accuracy: 0.3909\n",
      "2025-05-04 15:41:06,519 - INFO - Saved best model at epoch 2 with val accuracy: 0.3909\n",
      "Epoch 3 - Training: 100%|██████████| 744/744 [07:53<00:00,  1.57it/s]\n",
      "2025-05-04 15:48:59,875 - INFO - Epoch 3 - Train Loss: 2.1657, Train Accuracy: 0.2824\n",
      "Epoch 3 - Validation: 100%|██████████| 187/187 [00:55<00:00,  3.39it/s]\n",
      "2025-05-04 15:49:55,068 - INFO - Epoch 3 - Validation Loss: 1.4360, Validation Accuracy: 0.4218\n",
      "2025-05-04 15:49:55,457 - INFO - Saved best model at epoch 3 with val accuracy: 0.4218\n",
      "Epoch 4 - Training: 100%|██████████| 744/744 [07:52<00:00,  1.58it/s]\n",
      "2025-05-04 15:57:47,640 - INFO - Epoch 4 - Train Loss: 1.9765, Train Accuracy: 0.3269\n",
      "Epoch 4 - Validation: 100%|██████████| 187/187 [00:55<00:00,  3.39it/s]\n",
      "2025-05-04 15:58:42,841 - INFO - Epoch 4 - Validation Loss: 1.3451, Validation Accuracy: 0.4647\n",
      "2025-05-04 15:58:43,233 - INFO - Saved best model at epoch 4 with val accuracy: 0.4647\n",
      "Epoch 5 - Training: 100%|██████████| 744/744 [07:52<00:00,  1.57it/s]\n",
      "2025-05-04 16:06:35,950 - INFO - Epoch 5 - Train Loss: 1.7951, Train Accuracy: 0.3585\n",
      "Epoch 5 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.64it/s]\n",
      "2025-05-04 16:07:46,905 - INFO - Epoch 5 - Validation Loss: 1.3114, Validation Accuracy: 0.4950\n",
      "2025-05-04 16:07:47,297 - INFO - Saved best model at epoch 5 with val accuracy: 0.4950\n",
      "Epoch 6 - Training: 100%|██████████| 744/744 [07:50<00:00,  1.58it/s]\n",
      "2025-05-04 16:15:37,654 - INFO - Epoch 6 - Train Loss: 1.6590, Train Accuracy: 0.3933\n",
      "Epoch 6 - Validation: 100%|██████████| 187/187 [00:55<00:00,  3.37it/s]\n",
      "2025-05-04 16:16:33,220 - INFO - Epoch 6 - Validation Loss: 1.3095, Validation Accuracy: 0.4701\n",
      "Epoch 7 - Training: 100%|██████████| 744/744 [07:50<00:00,  1.58it/s]\n",
      "2025-05-04 16:24:23,856 - INFO - Epoch 7 - Train Loss: 1.5485, Train Accuracy: 0.4289\n",
      "Epoch 7 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.63it/s]\n",
      "2025-05-04 16:25:34,840 - INFO - Epoch 7 - Validation Loss: 1.2509, Validation Accuracy: 0.5218\n",
      "2025-05-04 16:25:35,235 - INFO - Saved best model at epoch 7 with val accuracy: 0.5218\n",
      "Epoch 8 - Training: 100%|██████████| 744/744 [07:52<00:00,  1.58it/s]\n",
      "2025-05-04 16:33:27,558 - INFO - Epoch 8 - Train Loss: 1.4439, Train Accuracy: 0.4491\n",
      "Epoch 8 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.64it/s]\n",
      "2025-05-04 16:34:38,477 - INFO - Epoch 8 - Validation Loss: 1.2368, Validation Accuracy: 0.5312\n",
      "2025-05-04 16:34:38,862 - INFO - Saved best model at epoch 8 with val accuracy: 0.5312\n",
      "Epoch 9 - Training: 100%|██████████| 744/744 [07:55<00:00,  1.56it/s]\n",
      "2025-05-04 16:42:34,855 - INFO - Epoch 9 - Train Loss: 1.3518, Train Accuracy: 0.4919\n",
      "Epoch 9 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.63it/s]\n",
      "2025-05-04 16:43:45,847 - INFO - Epoch 9 - Validation Loss: 1.2058, Validation Accuracy: 0.5480\n",
      "2025-05-04 16:43:46,250 - INFO - Saved best model at epoch 9 with val accuracy: 0.5480\n",
      "Epoch 10 - Training: 100%|██████████| 744/744 [07:54<00:00,  1.57it/s]\n",
      "2025-05-04 16:51:40,355 - INFO - Epoch 10 - Train Loss: 1.3009, Train Accuracy: 0.5123\n",
      "Epoch 10 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.64it/s]\n",
      "2025-05-04 16:52:51,217 - INFO - Epoch 10 - Validation Loss: 1.2244, Validation Accuracy: 0.5353\n",
      "Epoch 11 - Training: 100%|██████████| 744/744 [07:53<00:00,  1.57it/s]\n",
      "2025-05-04 17:00:44,478 - INFO - Epoch 11 - Train Loss: 1.2590, Train Accuracy: 0.5188\n",
      "Epoch 11 - Validation: 100%|██████████| 187/187 [01:11<00:00,  2.63it/s]\n",
      "2025-05-04 17:01:55,630 - INFO - Epoch 11 - Validation Loss: 1.2249, Validation Accuracy: 0.5473\n",
      "Epoch 12 - Training: 100%|██████████| 744/744 [07:54<00:00,  1.57it/s]\n",
      "2025-05-04 17:09:49,710 - INFO - Epoch 12 - Train Loss: 1.2068, Train Accuracy: 0.5583\n",
      "Epoch 12 - Validation: 100%|██████████| 187/187 [00:55<00:00,  3.39it/s]\n",
      "2025-05-04 17:10:44,920 - INFO - Epoch 12 - Validation Loss: 1.1785, Validation Accuracy: 0.5749\n",
      "2025-05-04 17:10:45,318 - INFO - Saved best model at epoch 12 with val accuracy: 0.5749\n",
      "Epoch 13 - Training: 100%|██████████| 744/744 [07:53<00:00,  1.57it/s]\n",
      "2025-05-04 17:18:38,901 - INFO - Epoch 13 - Train Loss: 1.1542, Train Accuracy: 0.5731\n",
      "Epoch 13 - Validation: 100%|██████████| 187/187 [00:55<00:00,  3.37it/s]\n",
      "2025-05-04 17:19:34,463 - INFO - Epoch 13 - Validation Loss: 1.1741, Validation Accuracy: 0.5729\n",
      "Epoch 14 - Training: 100%|██████████| 744/744 [07:52<00:00,  1.58it/s]\n",
      "2025-05-04 17:27:26,663 - INFO - Epoch 14 - Train Loss: 1.1412, Train Accuracy: 0.5679\n",
      "Epoch 14 - Validation: 100%|██████████| 187/187 [01:10<00:00,  2.64it/s]\n",
      "2025-05-04 17:28:37,578 - INFO - Epoch 14 - Validation Loss: 1.1780, Validation Accuracy: 0.5776\n",
      "2025-05-04 17:28:37,982 - INFO - Saved best model at epoch 14 with val accuracy: 0.5776\n",
      "Epoch 15 - Training: 100%|██████████| 744/744 [07:55<00:00,  1.56it/s]\n",
      "2025-05-04 17:36:33,869 - INFO - Epoch 15 - Train Loss: 1.1080, Train Accuracy: 0.5865\n",
      "Epoch 15 - Validation: 100%|██████████| 187/187 [01:11<00:00,  2.63it/s]\n",
      "2025-05-04 17:37:44,956 - INFO - Epoch 15 - Validation Loss: 1.1844, Validation Accuracy: 0.5668\n",
      "2025-05-04 17:37:45,020 - INFO - Predicted emotion for audio-emotion-dataset2\\1001_DFA_ANG_XX.wav: ANG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import librosa\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from transformers import get_scheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 16000\n",
    "MIN_DURATION = 0.1\n",
    "MAX_DURATION = 10.0\n",
    "MIN_AMPLITUDE = 1e-6\n",
    "MAX_LENGTH = 16000\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "# Emotion to integer mapping for CREMA-D\n",
    "emotion_map = {\n",
    "    \"ANG\": 0,  # Anger\n",
    "    \"DIS\": 1,  # Disgust\n",
    "    \"FEA\": 2,  # Fear\n",
    "    \"HAP\": 3,  # Happy\n",
    "    \"NEU\": 4,  # Neutral\n",
    "    \"SAD\": 5,  # Sad\n",
    "}\n",
    "\n",
    "# Validate audio file\n",
    "def validate_audio_file(file_path, processor):\n",
    "    try:\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            raise ValueError(\"Empty file\")\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if waveform.numel() == 0:\n",
    "            raise ValueError(\"Empty waveform\")\n",
    "        duration = waveform.shape[1] / sr\n",
    "        if duration < MIN_DURATION or duration > MAX_DURATION:\n",
    "            raise ValueError(f\"Invalid duration: {duration}s\")\n",
    "        waveform_np = waveform.squeeze(0).numpy()\n",
    "        max_amplitude = np.max(np.abs(waveform_np))\n",
    "        mean_amplitude = np.mean(np.abs(waveform_np))\n",
    "        if max_amplitude < MIN_AMPLITUDE:\n",
    "            raise ValueError(f\"Waveform amplitude too low: max={max_amplitude}, mean={mean_amplitude}\")\n",
    "        inputs = processor(waveform_np, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "        if inputs.input_values.numel() == 0 or torch.isnan(inputs.input_values).any():\n",
    "            raise ValueError(f\"Processor returned invalid input: numel={inputs.input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "        logger.debug(f\"Validated {file_path}: SR={sr}, Duration={duration}s, MaxAmplitude={max_amplitude}, MeanAmplitude={mean_amplitude}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Validation failed for {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Extract hand-crafted features using librosa\n",
    "def extract_audio_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T, axis=0)\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y).T, axis=0)\n",
    "        rms = np.mean(librosa.feature.rms(y=y).T, axis=0)\n",
    "        features = np.concatenate((mfcc, chroma, contrast, zcr, rms), axis=0)\n",
    "        if features.shape[0] != 34:\n",
    "            raise ValueError(f\"Expected 34 features, got {features.shape[0]}\")\n",
    "        logger.debug(f\"Extracted hand-crafted features for {file_path}: Shape={features.shape}\")\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting hand-crafted features for {file_path}: {e}\")\n",
    "        return np.zeros(34)\n",
    "\n",
    "# Extract Wav2Vec2 features\n",
    "def extract_wav2vec2_features(file_path, processor, sample_rate=16000, max_length=16000):\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if waveform.numel() == 0:\n",
    "            raise ValueError(\"Empty waveform\")\n",
    "        if sr != sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "        waveform_np = waveform.squeeze(0).numpy()\n",
    "        inputs = processor(waveform_np, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        input_values = inputs['input_values'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        if input_values.numel() == 0 or torch.isnan(input_values).any():\n",
    "            raise ValueError(f\"Processor returned invalid input: numel={input_values.numel()}, has_nan={torch.isnan(inputs.input_values).any()}\")\n",
    "        logger.debug(f\"Extracted Wav2Vec2 features for {file_path}: InputShape={input_values.shape}, MaskShape={attention_mask.shape}\")\n",
    "        return input_values, attention_mask\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting Wav2Vec2 features for {file_path}: {e}\")\n",
    "        return torch.zeros(max_length), torch.zeros(max_length)\n",
    "\n",
    "# Custom Dataset\n",
    "class CREMADataset(Dataset):\n",
    "    def __init__(self, audio_dir, processor, emotion_map, file_paths=None, labels=None, sample_rate=16000):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.processor = processor\n",
    "        self.emotion_map = emotion_map\n",
    "        self.sample_rate = sample_rate\n",
    "        self.file_paths = file_paths or []\n",
    "        self.labels = labels or []\n",
    "        self.invalid_files = []\n",
    "\n",
    "        if not file_paths or not labels:\n",
    "            for file_name in os.listdir(audio_dir):\n",
    "                if file_name.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(audio_dir, file_name)\n",
    "                    parts = file_name.split('_')\n",
    "                    if len(parts) < 3:\n",
    "                        logger.warning(f\"Invalid filename format: {file_name}\")\n",
    "                        continue\n",
    "                    emotion = parts[2]\n",
    "                    label = self.emotion_map.get(emotion, -1)\n",
    "                    if label != -1 and validate_audio_file(file_path, processor):\n",
    "                        self.file_paths.append(file_path)\n",
    "                        self.labels.append(label)\n",
    "                    else:\n",
    "                        self.invalid_files.append(file_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.normpath(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            wav2vec_features, attention_mask = extract_wav2vec2_features(file_path, self.processor, self.sample_rate, MAX_LENGTH)\n",
    "            hand_crafted_features = extract_audio_features(file_path)\n",
    "            return {\n",
    "                'input_values': wav2vec_features.clone().detach(),\n",
    "                'attention_mask': attention_mask.clone().detach(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long),\n",
    "                'hand_crafted_features': torch.tensor(hand_crafted_features, dtype=torch.float)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {file_path}: {e}\")\n",
    "            self.invalid_files.append(file_path)\n",
    "            return None\n",
    "\n",
    "# Custom Model\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=6):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            wav2vec_model_name, num_labels=num_labels, ignore_mismatched_sizes=True, output_hidden_states=True\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 34, 256),  # 768 from Wav2Vec2 hidden states + 34 from hand-crafted features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask, hand_crafted_features):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, 768)\n",
    "        pooled_features = hidden_states.mean(dim=1)  # Shape: (batch_size, 768)\n",
    "        combined_features = torch.cat((pooled_features, hand_crafted_features), dim=1)  # Shape: (batch_size, 768 + 34)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits\n",
    "\n",
    "# Load processor and dataset\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "audio_dir = \"audio-emotion-dataset2\"\n",
    "dataset = CREMADataset(audio_dir=audio_dir, processor=processor, emotion_map=emotion_map)\n",
    "\n",
    "# Split dataset\n",
    "train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "    dataset.file_paths, dataset.labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_dataset = CREMADataset(audio_dir, processor, emotion_map, train_file_paths, train_labels)\n",
    "val_dataset = CREMADataset(audio_dir, processor, emotion_map, val_file_paths, val_labels)\n",
    "\n",
    "# Filter out None samples\n",
    "def filter_dataset(dataset):\n",
    "    valid_indices = []\n",
    "    valid_data = []\n",
    "    for idx in range(len(dataset)):\n",
    "        item = dataset[idx]\n",
    "        if item is not None:\n",
    "            valid_indices.append(idx)\n",
    "            valid_data.append(item)\n",
    "    if not valid_data:\n",
    "        logger.error(f\"No valid data after filtering. Invalid files: {dataset.invalid_files}\")\n",
    "        raise ValueError(f\"No valid data after filtering. Check invalid files: {dataset.invalid_files[:10]}...\")\n",
    "    dataset.file_paths = [dataset.file_paths[i] for i in valid_indices]\n",
    "    dataset.labels = [dataset.labels[i] for i in valid_indices]\n",
    "    dataset.invalid_files = list(set(dataset.invalid_files))\n",
    "    logger.warning(f\"Invalid files: {dataset.invalid_files}\")\n",
    "    return valid_data\n",
    "\n",
    "train_dataset = filter_dataset(train_dataset)\n",
    "val_dataset = filter_dataset(val_dataset)\n",
    "\n",
    "# Create DataLoaders\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    input_values = [item['input_values'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    hand_crafted_features = [item['hand_crafted_features'] for item in batch]\n",
    "    input_values = pad_sequence(input_values, batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    hand_crafted_features = torch.stack(hand_crafted_features, dim=0)\n",
    "    return {\n",
    "        'input_values': input_values,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'hand_crafted_features': hand_crafted_features\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmotionRecognitionModel(wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=len(emotion_map))\n",
    "model.to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=num_training_steps\n",
    ")\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = \"wav2vec2_emotion_classifier_best_v2\"\n",
    "os.makedirs(best_model_path, exist_ok=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        inputs = batch['input_values']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        hand_crafted_features = batch['hand_crafted_features']\n",
    "        logger.debug(f\"Batch shapes: input_values={inputs.shape}, attention_mask={attention_mask.shape}, hand_crafted_features={hand_crafted_features.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, hand_crafted_features=hand_crafted_features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        correct_preds += (preds == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    logger.info(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Validation\"):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            inputs = batch['input_values']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            hand_crafted_features = batch['hand_crafted_features']\n",
    "            outputs = model(inputs, attention_mask=attention_mask, hand_crafted_features=hand_crafted_features)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "    val_accuracy = correct_preds / total_preds\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    logger.info(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(best_model_path, \"model.pt\"))\n",
    "        processor.save_pretrained(best_model_path)\n",
    "        logger.info(f\"Saved best model at epoch {epoch+1} with val accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Predict emotion for a single file\n",
    "def predict_emotion(file_path, model, processor):\n",
    "    model.eval()\n",
    "    try:\n",
    "        wav2vec_features, attention_mask = extract_wav2vec2_features(file_path, processor, SAMPLE_RATE, MAX_LENGTH)\n",
    "        hand_crafted_features = extract_audio_features(file_path)\n",
    "        inputs = {\n",
    "            'input_values': wav2vec_features.unsqueeze(0).to(device),\n",
    "            'attention_mask': attention_mask.unsqueeze(0).to(device),\n",
    "            'hand_crafted_features': torch.tensor(hand_crafted_features, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs, dim=-1).item()\n",
    "        return {v: k for k, v in emotion_map.items()}[pred]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error predicting {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test prediction\n",
    "sample_file = os.path.join(audio_dir, \"1001_DFA_ANG_XX.wav\")\n",
    "logger.info(f\"Predicted emotion for {sample_file}: {predict_emotion(sample_file, model, processor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae05d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\my_env\\Lib\\site-packages\\transformers\\configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_15260\\1822795567.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"wav2vec2_emotion_classifier_best_v2/model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmotionRecognitionModel(\n",
       "  (wav2vec2): Wav2Vec2ForSequenceClassification(\n",
       "    (wav2vec2): Wav2Vec2Model(\n",
       "      (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Wav2Vec2GroupNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): Wav2Vec2FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): Wav2Vec2Encoder(\n",
       "        (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (padding): Wav2Vec2SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "            (attention): Wav2Vec2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): Wav2Vec2FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projector): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (classifier): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=802, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Model\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=6):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            wav2vec_model_name, num_labels=num_labels, ignore_mismatched_sizes=True, output_hidden_states=True\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 34, 256),  # 768 from Wav2Vec2 hidden states + 34 from hand-crafted features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask, hand_crafted_features):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, 768)\n",
    "        pooled_features = hidden_states.mean(dim=1)  # Shape: (batch_size, 768)\n",
    "        combined_features = torch.cat((pooled_features, hand_crafted_features), dim=1)  # Shape: (batch_size, 768 + 34)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits\n",
    "\n",
    "emotion_map = {\n",
    "    \"ANG\": 0,  # Anger\n",
    "    \"DIS\": 1,  # Disgust\n",
    "    \"FEA\": 2,  # Fear\n",
    "    \"HAP\": 3,  # Happy\n",
    "    \"NEU\": 4,  # Neutral\n",
    "    \"SAD\": 5,  # Sad\n",
    "}\n",
    "    \n",
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# First load the model architecture (config will be default or custom if you have config.json)\n",
    "model = EmotionRecognitionModel(wav2vec_model_name=\"superb/wav2vec2-base-superb-er\", num_labels=len(emotion_map))\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"wav2vec2_emotion_classifier_best_v2/model.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577348d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 57.76%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Inference loop\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        attention_mask = batch.get('attention_mask')\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        hand_crafted_features = batch['hand_crafted_features'].to(device)\n",
    "\n",
    "        logits = model(input_values=input_values,\n",
    "                       attention_mask=attention_mask,\n",
    "                       hand_crafted_features=hand_crafted_features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "# best  Validation Accuracy: 55.14%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d8614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5664015518066707"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "val_f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b55f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.71      0.78      0.74       237\n",
      "         DIS       0.54      0.29      0.38       268\n",
      "         FEA       0.50      0.52      0.51       246\n",
      "         HAP       0.61      0.53      0.57       259\n",
      "         NEU       0.59      0.74      0.66       210\n",
      "         SAD       0.52      0.65      0.58       269\n",
      "\n",
      "    accuracy                           0.58      1489\n",
      "   macro avg       0.58      0.59      0.57      1489\n",
      "weighted avg       0.58      0.58      0.57      1489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [label for label, idx in sorted(emotion_map.items(), key=lambda x: x[1])]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb91a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:49:33,697 - INFO - NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "labels = list(emotion_map.values()) \n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
